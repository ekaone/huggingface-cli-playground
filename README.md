<div align="center"><strong>Machine Learning playground.</strong></div>
<div align="center">AI-powerd by Hugging Face.</div>
<br />
<div align="center">
<a href="https://huggingface.co/">Hugging Face</a> 
<span> Â· </span>
<a href="https://github.com/terkelg/prompts#readme">Prompts CLI</a> 
</div>

<br />
<div align="center">
  <sub>Cooked by <a href="https://twitter.com/twekaone">Eka Prasetia</a> ğŸ‘¨â€ğŸ³</sub>
</div>

<br />

## Installation

```sh
git clone https://github.com/ekaone/huggingface-cli-playground.git
cd huggingface-cli-playground
npm install
npm start
```

## Tasks and Models

ğŸ”¥ **Text Generation**
- [X] [`gpt2`](https://huggingface.co/gpt2) GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion
- [X] [`bigscience/bloom-560m`](https://huggingface.co/bigscience/bloom-560m) - BigScience Large Open-science Open-access Multilingual Language Model
- [ ] [`distilgpt2`](https://huggingface.co/distilgpt2) - DistilGPT2 (short for Distilled-GPT2) is an English-language model pre-trained with the supervision

ğŸ”© **Summarization**
- [ ] [`facebook/bart-large-cnn`](https://huggingface.co/facebook/bart-large-cnn) - BART model pre-trained on English language, and fine-tuned on CNN Daily Mail
- [ ] [`philschmid/bart-large-cnn-samsum`](https://huggingface.co/philschmid/bart-large-cnn-samsum) - This model was trained using Amazon SageMaker and the new Hugging Face Deep Learning container

## Getting Started

Run this command to start the CLI, choose the model and start playing with it.

